# Jupyter Notebook

import gym
import matplotlib.pyplot as plt
from n_step_rl_algorithm import NStepRLAlgorithm

# Define environments
environments = ['Blackjack-v0', 'Taxi-v3', 'CliffWalking-v0', 'FrozenLake-v1']

# Helper function to plot value functions, policy, and rewards
def plot_results(algo, environment, num_episodes):
    # Placeholder for plotting logic
    pass

# Markdown cell for Task III - Report
report_text = """
# Task III - Report

## Properties Explanation:

### Blackjack Environment:
The goal of this task is to train an agent to play Blackjack optimally against a fixed policy dealer. 
The state space consists of the current sum of player's cards, the dealer's face-up card, 
and whether the player holds a usable Ace. The action space includes 'hit' or 'stick.' 
Rewards are +1 for winning, -1 for losing, and 0 for a draw. The optimal policy should enable 
the player to win with a high probability against the fixed policy dealer.

### Taxi Environment:
The task is to accomplish the taxi domain optimally from any initial/final state. 
The state space represents the taxi's location, the passenger's location, and the destination. 
The action space includes moving in different directions and picking up/dropping off the passenger. 
Rewards are assigned based on actions and reaching the destination. The optimal policy 
should efficiently navigate the taxi to any given initial/final state.

### Cliff Walking Environment:
The goal is to accomplish the task from any initial/final state in a cliff walking scenario. 
The state space represents the agent's position on a grid. The action space consists of moving 
in four directions. Rewards are assigned based on actions, and falling off the cliff incurs a large negative reward. 
The optimal policy should navigate the agent safely to the goal from any initial/final state.

### Frozen Lake Environment:
The optimal policy is sought in the Frozen Lake environment, where the agent must navigate a frozen lake to reach a goal. 
The state space represents the agent's position on the grid, and the action space includes moving in four directions. 
Rewards are assigned based on actions, and falling into a hole incurs a negative reward. The optimal policy 
should guide the agent safely to the goal from any initial/final state.

## Parameter Selection Explanation:

The hyperparameters for the RL algorithm include the discount factor (gamma), learning rate (alpha), 
and the n-step value. The discount factor is set to 0.9, encouraging the agent to consider future rewards. 
The learning rate is set to 0.1, controlling the weight given to new information. The n-step value is adjusted 
from 1 to infinity to implement various algorithms, ranging from one-step TD to Monte Carlo. 
These values are selected through experimentation and are crucial for the convergence and 
performance of the RL algorithm in different environments.
"""

# Code cell for Task III - Plotting Logic
for env in environments:
    plot_results(algo=NStepRLAlgorithm(n=3, environment=env), environment=env, num_episodes=1000)
