{
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    },
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "Blackjack Environment:\n\nProperties:\nGoal: The goal in Blackjack is to have a hand value as close to 21 as possible without exceeding it. The agent aims to win with a high probability against a fixed policy dealer.\nState Space: The state space includes the current sum of the player's cards, the dealer's face-up card, and whether the player has a usable ace.\nAction Space: The action space consists of \"Hit\" or \"Stick\" (take another card or end the turn).\nReward: The agent receives a reward of +1 for winning, -1 for losing, and 0 for a draw.\nParameter Selection:\nFor the Blackjack environment, we tuned the learning rate (alpha) to 0.01 and the discount factor (gamma) to 0.9 based on experimentation.\nPlots:\nInclude plots for value functions, policies, and the sum of rewards over episodes.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Taxi Environment:\n\nProperties:\nGoal: The optimal policy aims to accomplish the task from any initial/final state in the Taxi environment.\nState Space: The state space represents the taxi's location, passenger location, and destination location.\nAction Space: The action space includes moving in cardinal directions and picking up/dropping off a passenger.\nReward: The agent receives a positive reward for a successful drop-off and a negative reward for illegal actions.\nParameter Selection:\nFor the Taxi environment, we adjusted the learning rate (alpha) to 0.1 and the discount factor (gamma) to 0.95.\nPlots:\nInclude plots for value functions, policies, and the sum of rewards over episodes.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Cliff Walking Environment:\n\nProperties:\nGoal: The optimal policy accomplishes the task from any initial/final state in the Cliff Walking environment.\nState Space: The state space represents the agent's position in a grid world.\nAction Space: The action space includes moving in cardinal directions.\nReward: The agent receives a negative reward for falling off the cliff and a positive reward for reaching the goal.\nParameter Selection:\nFor Cliff Walking, we set the learning rate (alpha) to 0.05 and the discount factor (gamma) to 0.99.\nPlots:\nInclude plots for value functions, policies, and the sum of rewards over episodes.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Frozen Lake Environment:\n\nProperties:\nGoal: The optimal policy aims to navigate the Frozen Lake to reach the goal.\nState Space: The state space represents the agent's position on the frozen lake.\nAction Space: The action space includes moving in cardinal directions.\nReward: The agent receives a positive reward for reaching the goal and a negative reward for falling into a hole.\nParameter Selection:\nFor Frozen Lake, we set the learning rate (alpha) to 0.02 and the discount factor (gamma) to 0.98.\nPlots:\nInclude plots for value functions, policies, and the sum of rewards over episodes.",
      "metadata": {}
    }
  ]
}